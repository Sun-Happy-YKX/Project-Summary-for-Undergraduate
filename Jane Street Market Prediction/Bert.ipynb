{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import janestreet\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dt.fread('/kaggle/input/jane-street-market-prediction/train.csv') #用datarable可以加速读取csv文件\n",
    "data = data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reduce Memory Usage by 50%\n",
    "https://www.kaggle.com/tomwarrens/nan-values-depending-on-time-of-day\n",
    "\"\"\"\n",
    "\n",
    "## Reduce Memory\n",
    "\n",
    "def reduce_memory_usage(df):\n",
    "    \n",
    "    start_memory = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage of dataframe is {start_memory} MB\")\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != 'object':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            \n",
    "            else:\n",
    "#                 reducing float16 for calculating numpy.nanmean\n",
    "#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "#                     df[col] = df[col].astype(np.float16)\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "    \n",
    "    end_memory = df.memory_usage().sum() / 1024**2\n",
    "    print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n",
    "    print(f\"Reduced by {100 * (start_memory - end_memory) / start_memory} % \")\n",
    "    return df\n",
    "\n",
    "data = reduce_memory_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[(data['weight'] == 0) | (data['date'] <= 85)].index) #删除前85天的数据和weight等于0的数据\n",
    "ignore_columns = ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp','ts_id','date']\n",
    "features = [col for col in data.columns if col not in ignore_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(data.mean()) #平均值填充\n",
    "data['action'] = (data['resp'] > 0).astype('int') #resp大于0使action取1，也就是买入\n",
    "data = data.drop(columns=ignore_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timeseries_Dataset(Dataset): \n",
    "    \"\"\"\n",
    "    Custom Dataset subclass.\n",
    "    Serves as input to DataLoader to transform X\n",
    "      into sequence data using rolling window.\n",
    "    DataLoader using this dataset will output batches\n",
    "      of `(batch_size, seq_len, n_features)` shape.\n",
    "    Suitable as an input to RNNs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int = 32):\n",
    "        self.X = torch.tensor(X).float()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.__len__() - (self.seq_len - 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x': torch.tensor(self.X[index:index + self.seq_len], dtype=torch.float),\n",
    "                'y': torch.tensor(self.y[index + self.seq_len - 1], dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(1, 2))\n",
    "\n",
    "        # if mask is not None:\n",
    "        #     attn = attn.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        output = torch.matmul(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multi-Head Attention module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "        batch_size = q.size(0)\n",
    "        residual = q\n",
    "\n",
    "        # Pass through the pre-attention projection: b x lq x (n*dv)\n",
    "        # Separate different heads: b x lq x n x dv\n",
    "        q = self.w_qs(q)\n",
    "        k = self.w_ks(k)\n",
    "        v = self.w_vs(v)\n",
    "\n",
    "        # Transpose for attention dot product: b x n x lq x dv\n",
    "        # q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        # if mask is not None:\n",
    "        #     mask = mask.unsqueeze(1)   # For head axis broadcasting.\n",
    "\n",
    "        q = q.view(n_head * batch_size, -1, d_k)\n",
    "        k = q.view(n_head * batch_size, -1, d_k)\n",
    "        v = q.view(n_head * batch_size, -1, d_v)\n",
    "\n",
    "        q, attn = self.attention(q, k, v)\n",
    "\n",
    "        q = q.view(batch_size, -1, n_head * d_k)\n",
    "        # Transpose to move the head dimension back: b x lq x n x dv\n",
    "        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n",
    "        q = self.dropout(self.fc(q))\n",
    "        q += residual\n",
    "\n",
    "        q = self.layer_norm(q)\n",
    "\n",
    "        return q, attn\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    ''' A two-feed-forward-layer module '''\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n",
    "        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    ''' Compose with two layers '''\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "        self.last_linear = nn.Linear(d_model, 2)\n",
    "\n",
    "    def forward(self, enc_input):\n",
    "        enc_output, attn = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        enc_output = self.last_linear(enc_output)\n",
    "        return enc_output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 4096\n",
    "lr = 0.0005\n",
    "d_model = 131\n",
    "d_inner = 89\n",
    "n_head = 8\n",
    "d_k = 45\n",
    "d_v = 45\n",
    "\n",
    "seq_dim = 32\n",
    "target_column = 'action'\n",
    "\n",
    "feature_columns = data.columns[~data.columns.isin([target_column])]\n",
    "train, validation = data[:int(len(data) * 0.8)], data[int(len(data) * 0.2):]  #pandas数据前80%作训练，后20%作预测\n",
    "train_features, train_target = train[feature_columns], train[[target_column]]\n",
    "validation_features, validation_target = validation[feature_columns], validation[[target_column]]\n",
    "train_dataset = Timeseries_Dataset(X=train_features.values, y=train_target.values, seq_len=seq_dim) #转成numpy数据\n",
    "validation_dataset = Timeseries_Dataset(X=validation_features.values, y=validation_target.values, seq_len=seq_dim)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = '/kaggle/input/weight-lstm/best_30.pth' #???\n",
    "\n",
    "phase_training = True\n",
    "if os.path.exists(weight):\n",
    "    phase_training = False\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = EncoderLayer(d_model, d_inner, n_head, d_k, d_v)\n",
    "model = model.to(device)\n",
    "if phase_training:\n",
    "    iterations_per_epoch = len(train_loader)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    print('Start model training ...')\n",
    "    best_acc = 0.0\n",
    "    patience, trials = 100, 0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        for i, train_batch in enumerate(train_loader):\n",
    "            model.train()\n",
    "            features = train_batch['x'].to(device)\n",
    "            targets = train_batch['y'].to(device)\n",
    "            targets = torch.squeeze(targets)\n",
    "            preds = model(features)\n",
    "            loss = criterion(preds, targets) #？？？ok\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch} best model saved with loss: {loss:2.2}')\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        for valid_batch in validation_loader:\n",
    "            features = valid_batch['x'].to(device)\n",
    "            targets = valid_batch['y'].to(device)\n",
    "            targets = torch.squeeze(targets)\n",
    "            preds = model(features)\n",
    "            preds = F.log_softmax(preds, dim=1).argmax(dim=1) #输出结果一行两个特征【0,1】，取出较大的特征对应的列数0或者1作为结果\n",
    "            total += targets.size(0)\n",
    "            correct += (preds == targets).sum().item() #tensor类型转换成python数字\n",
    "\n",
    "        acc = correct / total\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch: {epoch:3d}. Loss: {loss.item():.4f}. Acc.: {acc:2.2%}')\n",
    "\n",
    "        if acc > best_acc:\n",
    "            trials = 0\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'best.pth') #？？？ok\n",
    "            print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(f'Early stopping on epoch {epoch}')\n",
    "                break\n",
    "    print('Training Complete !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if phase_training: #???ok\n",
    "    model.load_state_dict(torch.load('best.pth'))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(weight))\n",
    "    \n",
    "model.eval()\n",
    "X_test = None\n",
    "env = janestreet.make_env()\n",
    "env_iter = env.iter_test()\n",
    "for (test_df, pred_df) in tqdm(env_iter):\n",
    "    if test_df['weight'].item() > 0:\n",
    "        test_df = pd.DataFrame(test_df, columns=feature_columns)\n",
    "        test_df = test_df.fillna(test_df.mean())\n",
    "        if X_test is None:\n",
    "            X_test = np.concatenate([test_df for _ in range(seq_dim)],axis=0)\n",
    "        X_test = np.concatenate([X_test[1:], test_df] ,axis=0)\n",
    "        preds = model(torch.tensor(X_test[np.newaxis,:], dtype=torch.float).to(device))\n",
    "#         preds = preds.cpu().detach().numpy()\n",
    "#         action = ((test_df['weight'].values * preds[:, 1]) > 0).astype('int')\n",
    "        preds = F.log_softmax(preds, dim=1).argmax(dim=1)\n",
    "        action = preds.cpu().detach().numpy()\n",
    "        pred_df.action = action\n",
    "    else:\n",
    "        pred_df.action = 0\n",
    "    env.predict(pred_df)"
   ]
  }
 ]
}