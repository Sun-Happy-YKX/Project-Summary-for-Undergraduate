{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file /mnt/bert_model/chinese_L-12_H-768_A-12/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from pytorch_pretrained_bert import BertConfig\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from argparse import ArgumentParser\n",
    "import torch.multiprocessing as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BERT_MODEL_PATH = '/mnt/bert_model/chinese_L-12_H-768_A-12'\n",
    "\n",
    "bert_config = BertConfig('/mnt/bert_model/chinese_L-12_H-768_A-12/bert_config.json')\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/mnt/news_virus/train.csv'\n",
    "test_path = '/mnt/news_virus/test_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>picture_lists</th>\n",
       "      <th>category</th>\n",
       "      <th>ncw_label</th>\n",
       "      <th>fake_label</th>\n",
       "      <th>real_label</th>\n",
       "      <th>comment_2c</th>\n",
       "      <th>comment_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16973</th>\n",
       "      <td>3595572676621165</td>\n",
       "      <td>ã€å†›è½¦å°±æ˜¯äº†ä¸èµ·ï¼æœ‰æœ¬äº‹ä½ æ‰¾æˆ‘èŒ¬å•Šï¼Ÿã€‘ä¸€ç½‘å‹ï¼Œè·¯è¿‡å—äº¬å¸‚ä¸­å±±é™µå‰æ¹–è·¯å£ï¼Œçœ‹è§ä¸€æŒ‚å†›ç‰Œè½¦è¾†é€†å‘...</td>\n",
       "      <td>c98dea6573dfe82f87c222ca401235b5.jpg\\t4c88284d...</td>\n",
       "      <td>å†›äº‹</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>è½¦ç‰Œå·ï¼šå—K.20128\\tä¸‰å¹´å‰çš„äº‹æƒ…è¿˜æ‹¿å‡ºæ¥ç‚’ï¼Œç§€å…¬çŸ¥ä¸‹çº¿ä¹ˆ\\t</td>\n",
       "      <td>è½¦ç‰Œå·ï¼šå—K.20128\\tä¸‰å¹´å‰çš„äº‹æƒ…è¿˜æ‹¿å‡ºæ¥ç‚’ï¼Œç§€å…¬çŸ¥ä¸‹çº¿ä¹ˆ\\t[è½¬å‘]//@ç‹æœˆç´33...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                            content  \\\n",
       "16973  3595572676621165  ã€å†›è½¦å°±æ˜¯äº†ä¸èµ·ï¼æœ‰æœ¬äº‹ä½ æ‰¾æˆ‘èŒ¬å•Šï¼Ÿã€‘ä¸€ç½‘å‹ï¼Œè·¯è¿‡å—äº¬å¸‚ä¸­å±±é™µå‰æ¹–è·¯å£ï¼Œçœ‹è§ä¸€æŒ‚å†›ç‰Œè½¦è¾†é€†å‘...   \n",
       "\n",
       "                                           picture_lists category  ncw_label  \\\n",
       "16973  c98dea6573dfe82f87c222ca401235b5.jpg\\t4c88284d...       å†›äº‹          0   \n",
       "\n",
       "       fake_label  real_label                          comment_2c  \\\n",
       "16973           1           0  è½¦ç‰Œå·ï¼šå—K.20128\\tä¸‰å¹´å‰çš„äº‹æƒ…è¿˜æ‹¿å‡ºæ¥ç‚’ï¼Œç§€å…¬çŸ¥ä¸‹çº¿ä¹ˆ\\t   \n",
       "\n",
       "                                             comment_all  \n",
       "16973  è½¦ç‰Œå·ï¼šå—K.20128\\tä¸‰å¹´å‰çš„äº‹æƒ…è¿˜æ‹¿å‡ºæ¥ç‚’ï¼Œç§€å…¬çŸ¥ä¸‹çº¿ä¹ˆ\\t[è½¬å‘]//@ç‹æœˆç´33...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>picture_lists</th>\n",
       "      <th>category</th>\n",
       "      <th>comment_2</th>\n",
       "      <th>comment_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000408f5c5d17a3916f791ca340ce293</td>\n",
       "      <td>å„è·¯å£24å°æ—¶çº¢ç¯ï¼Œé—¯ä¸€æ¬¡6åˆ†200å…ƒï¼é€šçŸ¥ï¼Œå˜‰é±¼å¿åŸåŒºå†…ï¼Œäºä»Šæ—¥å‡Œæ™¨24åæ‰€æœ‰è·¯å£çº¢ç»¿ç¯ï¼Œ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ç–«æƒ…</td>\n",
       "      <td>ä¸é”™ğŸ˜Š\\t</td>\n",
       "      <td>ä¸é”™ğŸ˜Š\\t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  \\\n",
       "0  000408f5c5d17a3916f791ca340ce293   \n",
       "\n",
       "                                             content picture_lists category  \\\n",
       "0  å„è·¯å£24å°æ—¶çº¢ç¯ï¼Œé—¯ä¸€æ¬¡6åˆ†200å…ƒï¼é€šçŸ¥ï¼Œå˜‰é±¼å¿åŸåŒºå†…ï¼Œäºä»Šæ—¥å‡Œæ™¨24åæ‰€æœ‰è·¯å£çº¢ç»¿ç¯ï¼Œ...           NaN       ç–«æƒ…   \n",
       "\n",
       "  comment_2 comment_all  \n",
       "0     ä¸é”™ğŸ˜Š\\t       ä¸é”™ğŸ˜Š\\t  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan (empty boxes) with 0\n",
    "train_df = train_df.fillna({'content':'','picture_lists':'','category':'','ncw_label':0,'fake_label':0,'real_label':0,'comment_2c':'','comment_all':''})\n",
    "test_df = test_df.fillna({'content':'','picture_lists':'','category':'','comment_2c':'','comment_all':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.values\n",
    "test = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'train':[[train[i][4], train[i][5], train[i][6]] for i in range(len(train))]}\n",
    "contents = {'train':[train[i][1] for i in range(len(train))], 'test':[test[i][1] for i in range(len(test))]}\n",
    "contents_len = {'train':[len(train[i][1]) for i in range(len(train))], 'test':[len(test[i][1]) for i in range(len(test))]}\n",
    "subjects = {'train':[train[i][3] for i in range(len(train))], 'test':[test[i][3] for i in range(len(test))]}\n",
    "pics = {'train':[train[i][2] for i in range(len(train))], 'test':[test[i][2] for i in range(len(test))]}\n",
    "pics_t = []\n",
    "for i in range(len(train)):\n",
    "    if train[i][2] != '':\n",
    "        pics_t.append(len(train[i][2].strip('\\t').split('\\t')))\n",
    "    else:\n",
    "        pics_t.append(0)\n",
    "pics_te = []\n",
    "for i in range(len(test)):\n",
    "    if test[i][2] != '':\n",
    "        pics_te.append(len(test[i][2].strip('\\t').split('\\t')))\n",
    "    else:\n",
    "        pics_te.append(0)\n",
    "picsnum = {'train':pics_t, 'test':pics_te}\n",
    "comments_2 = {'train':[train[i][7] for i in range(len(train))], 'test':[test[i][4] for i in range(len(test))]}\n",
    "comments_all = {'train':[train[i][8] for i in range(len(train))], 'test':[test[i][5] for i in range(len(test))]}\n",
    "com_t = []\n",
    "for i in range(len(train)):\n",
    "    if train[i][8] != '':\n",
    "        com_t.append(len(train[i][8].strip('\\t').split('\\t')))\n",
    "    else:\n",
    "        com_t.append(0)\n",
    "com_te = []\n",
    "for i in range(len(test)):\n",
    "    if test[i][5] != '':\n",
    "        com_te.append(len(test[i][5].strip('\\t').split('\\t')))\n",
    "    else:\n",
    "        com_te.append(0)\n",
    "commentsnum = {'train':com_t, 'test':com_te}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2784\n",
      "1 203\n",
      "2 36\n",
      "3 39\n",
      "4 23\n",
      "5 7\n",
      "6 93\n",
      "7 6\n",
      "9 15\n"
     ]
    }
   ],
   "source": [
    "for each in set(picsnum['test']):\n",
    "    print(each, picsnum['test'].count(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pics_num\n",
    "picsnum_f = {'train':[0]*len(train), 'test':[0]*len(test)}\n",
    "for i in range(len(train)):\n",
    "    num = picsnum['train'][i] / 10.0\n",
    "    picsnum_f['train'][i] = [num for i in range(768)]\n",
    "\n",
    "for i in range(len(test)):\n",
    "    num = picsnum['test'][i] / 10.0\n",
    "    picsnum_f['test'][i] = [num for i in range(768)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1994 146\n"
     ]
    }
   ],
   "source": [
    "temp = contents_len['train']\n",
    "temp.sort()\n",
    "print(temp[0],temp[-1],temp[int(len(train)/5 * 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents_len\n",
    "contents_len_f = {'train':[0]*len(train), 'test':[0]*len(test)}\n",
    "for i in range(len(train)):\n",
    "    if contents_len['train'][i] < 200:\n",
    "        num = contents_len['train'][i] / 200.0\n",
    "        contents_len_f['train'][i] = [num for i in range(768)]\n",
    "    else:\n",
    "        contents_len_f['train'][i] = [0.9 for i in range(768)]\n",
    "\n",
    "for i in range(len(test)):\n",
    "    if contents_len['test'][i] < 200:\n",
    "        num = contents_len['test'][i] / 200.0\n",
    "        contents_len_f['test'][i] = [num for i in range(768)]\n",
    "    else:\n",
    "        contents_len_f['test'][i] = [0.9 for i in range(768)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, num_labels=3): # Change number of labels here.\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel.from_pretrained('/mnt/bert_model/chinese_L-12_H-768_A-12')\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        #self.fc1 = nn.Linear(config.hidden_size*2, 512)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "\n",
    "    '''def forward_once(self, x):\n",
    "        # Forward pass\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output'''\n",
    "\n",
    "    def forward_once(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        #logits = self.classifier(pooled_output)\n",
    "\n",
    "        return pooled_output\n",
    "\n",
    "    def forward(self, input_ids1, pics_num, cont_len):\n",
    "        # forward pass of input 1\n",
    "        output1 = self.forward_once(input_ids1, token_type_ids=None, attention_mask=None, labels=None)\n",
    "        \n",
    "        #print(out.shape)\n",
    "\n",
    "        # Multiply the credit score with the output after concatnation\n",
    "\n",
    "        #out = torch.add(pics_num, output1)\n",
    "        #out = torch.add(cont_len, out)\n",
    "\n",
    "        #out = self.fc1(out)\n",
    "        logits = self.classifier(output1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling:loading archive file /mnt/bert_model/chinese_L-12_H-768_A-12\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(vocab_size_or_config_json_file=22000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "\n",
    "model = BertForSequenceClassification(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,X_val, y_train, y_val  = cross_validation.train_test_split(train_data,train_target,test_size=0.4, random_state=0)\n",
    "X_train_all = [each.replace('\\t','').replace('\\u200b','').replace('#','').replace('@','') for each in contents['train']]\n",
    "y_train_all = labels['train']\n",
    "X_train_piscnum_all = picsnum_f['train']\n",
    "X_train_contentslen_all = contents_len_f['train']\n",
    "\n",
    "X_train = X_train_all[:int(len(train)*0.8)]\n",
    "y_train = y_train_all[:int(len(train)*0.8)]\n",
    "X_train_piscnum = X_train_piscnum_all[:int(len(train)*0.8)]\n",
    "X_train_contentslen = X_train_contentslen_all[:int(len(train)*0.8)]\n",
    "\n",
    "X_val = X_train_all[int(len(train)*0.8):]\n",
    "y_val = y_train_all[int(len(train)*0.8):]\n",
    "X_val_piscnum = X_train_piscnum_all[int(len(train)*0.8):]\n",
    "X_val_contentslen = X_train_contentslen_all[int(len(train)*0.8):]\n",
    "\n",
    "for i in range(len(contents['test'])):\n",
    "    if contents['test'][i] == '':\n",
    "        contents['test'][i] = 'None'\n",
    "X_test = [each.replace('\\t','').replace('\\u200b','').replace('#','').replace('@','') for each in contents['test']]\n",
    "X_test_piscnum = picsnum_f['test']\n",
    "X_test_contentslen = contents_len_f['test']\n",
    "\n",
    "max_seq_length_con = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:100]\n",
    "y_train = y_train[:100]\n",
    "X_train_piscnum = X_train_piscnum[:100]\n",
    "X_train_contentslen = X_train_contentslen[:100]\n",
    "\n",
    "X_val = X_val[:100]\n",
    "y_val = y_val[:100]\n",
    "X_val_piscnum = X_val_piscnum[:100]\n",
    "X_val_contentslen = X_val_contentslen[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 8\n",
    "batch_size_val = 12\n",
    "batch_size_test = 12\n",
    "\n",
    "# Train\n",
    "train_lists = [X_train, X_train_piscnum, X_train_contentslen, y_train]\n",
    "\n",
    "# Val\n",
    "val_lists = [X_val, X_val_piscnum, X_val_contentslen, y_val]\n",
    "\n",
    "# Test\n",
    "test_lists = [X_test, X_test_piscnum, X_test_contentslen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_dataset(Dataset):\n",
    "    def __init__(self,x_y_list, transform=None):\n",
    "\n",
    "        self.x_y_list = x_y_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        # Tokenize statements\n",
    "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
    "\n",
    "        if len(tokenized_review) > max_seq_length_con:\n",
    "            tokenized_review = tokenized_review[:max_seq_length_con]\n",
    "\n",
    "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
    "\n",
    "        padding = [0] * (max_seq_length_con - len(ids_review))\n",
    "\n",
    "        ids_review += padding\n",
    "\n",
    "        assert len(ids_review) == max_seq_length_con\n",
    "\n",
    "        #print(ids_review)\n",
    "        ids_review = torch.tensor(ids_review)\n",
    "\n",
    "        fakeness = self.x_y_list[3][index] # color\n",
    "        list_of_labels = [torch.from_numpy(np.array(fakeness))]\n",
    "\n",
    "        piscnum = self.x_y_list[1][index] # Credit score\n",
    "\n",
    "        #ones_768 = np.ones((768))\n",
    "        #credit_scr = credit_scr * ones_768\n",
    "        piscnum = torch.tensor(piscnum)\n",
    "        \n",
    "        contentslen = self.x_y_list[2][index] # Credit score\n",
    "\n",
    "        #ones_768 = np.ones((768))\n",
    "        #credit_scr = credit_scr * ones_768\n",
    "        contentslen = torch.tensor(contentslen)\n",
    "\n",
    "        return [ids_review, piscnum, contentslen], list_of_labels[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_dataset_test(Dataset):\n",
    "    def __init__(self,x_y_list, transform=None):\n",
    "\n",
    "        self.x_y_list = x_y_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        # Tokenize statements\n",
    "        tokenized_review = tokenizer.tokenize(self.x_y_list[0][index])\n",
    "\n",
    "        if len(tokenized_review) > max_seq_length_con:\n",
    "            tokenized_review = tokenized_review[:max_seq_length_con]\n",
    "\n",
    "        ids_review  = tokenizer.convert_tokens_to_ids(tokenized_review)\n",
    "\n",
    "        padding = [0] * (max_seq_length_con - len(ids_review))\n",
    "\n",
    "        ids_review += padding\n",
    "\n",
    "        assert len(ids_review) == max_seq_length_con\n",
    "\n",
    "        #print(ids_review)\n",
    "        ids_review = torch.tensor(ids_review)\n",
    "\n",
    "        piscnum = self.x_y_list[1][index] # Credit score\n",
    "\n",
    "        #ones_768 = np.ones((768))\n",
    "        #credit_scr = credit_scr * ones_768\n",
    "        piscnum = torch.tensor(piscnum)\n",
    "        \n",
    "        contentslen = self.x_y_list[2][index] # Credit score\n",
    "\n",
    "        #ones_768 = np.ones((768))\n",
    "        #credit_scr = credit_scr * ones_768\n",
    "        contentslen = torch.tensor(contentslen)\n",
    "\n",
    "        return [ids_review, piscnum, contentslen]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Preparing the data (Tokenize)\n",
    "training_dataset = text_dataset(x_y_list = train_lists)\n",
    "val_dataset = text_dataset(x_y_list = val_lists)\n",
    "test_dataset = text_dataset_test(x_y_list = test_lists)\n",
    "\n",
    "\n",
    "# Prepare the training dictionaries\n",
    "dataloaders_dict = {'train': torch.utils.data.DataLoader(training_dataset, batch_size=batch_size_train, shuffle=True, num_workers=0),\n",
    "                    'val':torch.utils.data.DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False, num_workers=0),\n",
    "                    'test':torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False, num_workers=0)\n",
    "                   }\n",
    "dataset_sizes = {'train':len(train_lists[0]),\n",
    "                'val':len(val_lists[0]),\n",
    "                'test':len(test_lists[0])}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    print('starting')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 100\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                #scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            fakeness_corrects = 0\n",
    "\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, fakeness in dataloaders_dict[phase]:\n",
    "\n",
    "                inputs1 = inputs[0] # News statement input\n",
    "                inputs2 = inputs[1] # Justification input\n",
    "                inputs3 = inputs[2] # Meta data input\n",
    "\n",
    "                inputs1 = inputs1.to(device)\n",
    "                inputs2 = inputs2.to(device)\n",
    "                inputs3 = inputs3.to(device)\n",
    "\n",
    "                fakeness = fakeness.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    #print(inputs)\n",
    "                    outputs = model(inputs1, inputs2, inputs3)\n",
    "\n",
    "                    outputs = F.softmax(outputs,dim=1)\n",
    "\n",
    "                    loss = criterion(outputs, torch.max(fakeness.float(), 1)[1])\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs1.size(0)\n",
    "\n",
    "\n",
    "                fakeness_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(fakeness, 1)[1])\n",
    "\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "            fakeness_acc = fakeness_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n",
    "            print('{} fakeness_acc: {:.4f}'.format(\n",
    "                phase, fakeness_acc))\n",
    "\n",
    "            # Saving training acc and loss for each epoch\n",
    "            if phase == 'train':\n",
    "                fakeness_acc1 = fakeness_acc.data\n",
    "                fakeness_acc1 = fakeness_acc1.cpu()\n",
    "                fakeness_acc1 = fakeness_acc1.numpy()\n",
    "                train_acc.append(fakeness_acc1)\n",
    "\n",
    "                #epoch_loss1 = epoch_loss.data\n",
    "                #epoch_loss1 = epoch_loss1.cpu()\n",
    "                #epoch_loss1 = epoch_loss1.numpy()\n",
    "                train_loss.append(epoch_loss)\n",
    "                \n",
    "                #print('labels\\n', fakeness)\n",
    "                #print('outputs\\n', outputs)\n",
    "                #print('torchmax\\n', torch.max(outputs, 1)[1])\n",
    "\n",
    "            else:\n",
    "                # Saving val acc and loss for each epoch\n",
    "                fakeness_acc1 = fakeness_acc.data\n",
    "                fakeness_acc1 = fakeness_acc1.cpu()\n",
    "                fakeness_acc1 = fakeness_acc1.numpy()\n",
    "                val_acc.append(fakeness_acc1)\n",
    "\n",
    "                #epoch_loss1 = epoch_loss.data\n",
    "                #epoch_loss1 = epoch_loss1.cpu()\n",
    "                #epoch_loss1 = epoch_loss1.numpy()\n",
    "                val_loss.append(epoch_loss)\n",
    "                \n",
    "                #print('labels\\n', fakeness)\n",
    "                #print('outputs\\n', outputs)\n",
    "                #print('torchmax\\n', torch.max(outputs, 1)[1])\n",
    "\n",
    "                if fakeness_acc > best_acc:\n",
    "                    print('Saving with accuracy of {}'.format(fakeness_acc),\n",
    "                          'improved over previous {}'.format(best_acc))\n",
    "                    best_acc = fakeness_acc\n",
    "                    \n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    torch.save(model.state_dict(), 'oneBERT_binary_focalloss.pth')\n",
    "\n",
    "        print('Time taken for epoch'+ str(epoch+1)+ ' is ' + str((time.time() - epoch_start)/60) + ' minutes')\n",
    "        print()\n",
    "        scheduler.step()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(float(best_acc)))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_acc, val_acc, train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Epoch 1/4\n",
      "----------\n",
      "train total loss: 1.1521 \n",
      "train fakeness_acc: 0.2800\n",
      "val total loss: 1.1061 \n",
      "val fakeness_acc: 0.4300\n",
      "Saving with accuracy of 0.43 improved over previous 0\n",
      "Time taken for epoch1 is 0.6261581381162008 minutes\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train total loss: 1.0654 \n",
      "train fakeness_acc: 0.4500\n",
      "val total loss: 1.0714 \n",
      "val fakeness_acc: 0.4800\n",
      "Saving with accuracy of 0.48 improved over previous 0.43\n",
      "Time taken for epoch2 is 0.6305120587348938 minutes\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train total loss: 1.0062 \n",
      "train fakeness_acc: 0.5400\n",
      "val total loss: 1.0459 \n",
      "val fakeness_acc: 0.4900\n",
      "Saving with accuracy of 0.49 improved over previous 0.48\n",
      "Time taken for epoch3 is 0.6366331775983175 minutes\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train total loss: 0.9407 \n",
      "train fakeness_acc: 0.6100\n",
      "val total loss: 1.0344 \n",
      "val fakeness_acc: 0.5100\n",
      "Saving with accuracy of 0.51 improved over previous 0.49\n",
      "Time taken for epoch4 is 0.6322222948074341 minutes\n",
      "\n",
      "Training complete in 2m 32s\n",
      "Best val Acc: 0.510000\n"
     ]
    }
   ],
   "source": [
    "lrlast = .0001\n",
    "lrmain = .00001\n",
    "optim1 = optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "\n",
    "   ])\n",
    "\n",
    "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 3 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n",
    "\n",
    "model_ft1, train_acc, val_acc, train_loss, val_loss = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "model.eval()\n",
    "\n",
    "valid_preds = []\n",
    "fakeness_corrects = 0\n",
    "\n",
    "for inputs in dataloaders_dict['test']:\n",
    "    \n",
    "    inputs1 = inputs[0] # News statement input\n",
    "    inputs2 = inputs[1] # Justification input\n",
    "    inputs3 = inputs[2] # Meta data input\n",
    "\n",
    "    inputs1 = inputs1.to(device)\n",
    "    inputs2 = inputs2.to(device)\n",
    "    inputs3 = inputs3.to(device)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    #optimizer.zero_grad()\n",
    "    \n",
    "    #print(inputs)\n",
    "    outputs = model(inputs1, inputs2, inputs3)\n",
    "\n",
    "    outputs = F.softmax(outputs,dim=1)\n",
    "    \n",
    "    valid_preds.extend(outputs.data.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
